{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGB model\n",
    "# Light Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "# 0:machine 1:human\n",
    "import json\n",
    "import random\n",
    "domain1 = []\n",
    "with open('../data/domain1_train.json', 'r') as json_file:\n",
    "    for line in json_file:\n",
    "        try:\n",
    "            json_data = json.loads(line)\n",
    "            domain1.append(json_data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "domain2 = []\n",
    "with open('../data/domain2_train.json', 'r') as json_file:\n",
    "    for line in json_file:\n",
    "        try:\n",
    "            json_data = json.loads(line)\n",
    "            domain2.append(json_data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing the data in domain2 \n",
    "# 0: machine 1:human\n",
    "# randomly choose 2150 number of label=0\n",
    "data_label_0 = [item for item in domain2 if item['label'] == 0]\n",
    "data_label_1 = [item for item in domain2 if item['label'] == 1]\n",
    "random.seed(42)  # set seed to make sure that we can see the same data for every iterations\n",
    "chosen_domain_2 = random.sample(data_label_0, 2150)\n",
    "final_domain_2 = data_label_1 + chosen_domain_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_domain = final_domain_2 + domain1\n",
    "# Adding a new feature called text length\n",
    "text_len = []\n",
    "for i in range(len(all_domain)):\n",
    "    text_len.append(len(all_domain[i]['text']))\n",
    "# Getting the original text data which are in token formatting\n",
    "old_text = []\n",
    "labels = []\n",
    "for i in range(len(all_domain)):\n",
    "    old_text.append(all_domain[i]['text'])\n",
    "    labels.append(all_domain[i]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent tokens into bow formatting\n",
    "data = old_text\n",
    "unique_integers = set()\n",
    "for integer_list in data:\n",
    "    unique_integers.update(integer_list)\n",
    "\n",
    "# changing all the unique_intergers into a list for storing the bow data\n",
    "vocabulary = sorted(list(unique_integers))\n",
    "\n",
    "\n",
    "# creating BOW representation\n",
    "bow_data = []\n",
    "for integer_list in data:\n",
    "    bow_vector = [integer_list.count(word) for word in vocabulary]\n",
    "    bow_data.append(bow_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score,confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Getting the features and the corresponding labels\n",
    "df = pd.DataFrame({'bow': bow_data, 'text_len': text_len})\n",
    "X = df[['bow','text_len']]\n",
    "y = labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9582, number of negative: 9458\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.135942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16451\n",
      "[LightGBM] [Info] Number of data points in the train set: 19040, number of used features: 4099\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503256 -> initscore=0.013025\n",
      "[LightGBM] [Info] Start training from score 0.013025\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[199]\tvalid_0's binary_logloss: 0.227935\n",
      "[LightGBM] [Info] Number of positive: 9582, number of negative: 9458\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16451\n",
      "[LightGBM] [Info] Number of data points in the train set: 19040, number of used features: 4099\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503256 -> initscore=0.013025\n",
      "[LightGBM] [Info] Start training from score 0.013025\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[199]\tvalid_0's binary_logloss: 0.227935\n",
      "Accuracy: 0.8974789915966387\n",
      "Confusion Matrix:\n",
      "[[2105  337]\n",
      " [ 151 2167]]\n"
     ]
    }
   ],
   "source": [
    "# Getting the train and testing data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# changing the features into np.array formatting \n",
    "X_train_bow = np.array(X_train['bow'].tolist())\n",
    "X_test_bow = np.array(X_test['bow'].tolist())\n",
    "\n",
    "# creating the lgb formatting data\n",
    "train_data = lgb.Dataset(X_train_bow, label=y_train)\n",
    "test_data = lgb.Dataset(X_test_bow, label=y_test, reference=train_data)\n",
    "\n",
    "# define the hyperparameters after fine tuning\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    # default value 31\n",
    "    'num_leaves': 50,\n",
    "    # default value 0.1\n",
    "    'learning_rate': 0.1,\n",
    "    # default value 100\n",
    "    'num_iterations':200,\n",
    "    'lambda_l1':0.0,\n",
    "    # default value 1.0\n",
    "    'feature_fraction': 0.6,\n",
    "    'early_stopping_rounds': 100\n",
    "}\n",
    "\n",
    "# Model training\n",
    "#num_round = 200\n",
    "bst = lgb.train(params, train_data,valid_sets=[test_data])\n",
    "# Find the best iteration which gives the best results \n",
    "best_iteration = bst.best_iteration\n",
    "\n",
    "bst = lgb.train(params, train_data,valid_sets=[test_data],num_boost_round=best_iteration)\n",
    "\n",
    "# Model predictions\n",
    "y_pred_prob = bst.predict(X_test_bow, num_iteration=bst.best_iteration)\n",
    "y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "confusion = confusion_matrix(y_test, y_pred_binary)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_binary)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Confusion Matrix:\\n{confusion}')\n",
    "print(f'ROC AUC: {roc_auc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better accuracy:\n",
    "\n",
    "Use large max_bin (may be slower)\n",
    "Use small learning_rate with large num_iterations\n",
    "Use large num_leaves(may cause over-fitting)\n",
    "Use bigger training data\n",
    "Try dart\n",
    "Try to use categorical feature directly\\\n",
    "\n",
    "To deal with over-fitting:\n",
    "Use small max_bin\n",
    "Use small num_leaves\n",
    "Use min_data_in_leaf and min_sum_hessian_in_leaf\n",
    "Use bagging by set bagging_fraction and bagging_freq\n",
    "Use feature sub-sampling by set feature_fraction\n",
    "Use bigger training data\n",
    "Try lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "Try max_depth to avoid growing deep tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the data and get results!!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing on the test dataset\n",
    "test = []\n",
    "with open('../data/test_set.json', 'r') as json_file:\n",
    "    for line in json_file:\n",
    "        try:\n",
    "            json_data = json.loads(line)\n",
    "            test.append(json_data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "# taking all the data out to get the predictions\n",
    "X_train_final = []\n",
    "for i in range(len(test)):\n",
    "    X_train_final.append(test[i]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4538\n"
     ]
    }
   ],
   "source": [
    "# check the individual word count for the testing data\n",
    "check_num = []\n",
    "for i in range(len(X_train_final)):\n",
    "    for value in X_train_final[i]:\n",
    "        check_num.append(value)\n",
    "print(len(set(check_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer the data into bow formatting\n",
    "# The number of features in data (4538) is not the same as it was in training data (5000).\n",
    "# Apply the word bank of the old text data which consist 5000 words \n",
    "data = old_text\n",
    "unique_integers = set()\n",
    "for integer_list in data:\n",
    "    unique_integers.update(integer_list)\n",
    "vocabulary = sorted(list(unique_integers))\n",
    "bow_data_final = []\n",
    "for integer_list in X_train_final: \n",
    "    bow_vector = [integer_list.count(word) for word in vocabulary]\n",
    "    bow_data_final.append(bow_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer into np.array formatt\n",
    "bow_data_final = np.array(bow_data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictions\n",
    "y_pred_final = bst.predict(bow_data_final, num_iteration=bst.best_iteration,predict_disable_shape_check=True)\n",
    "y_pred_binary_final = (y_pred_final > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write it into file \n",
    "len(y_pred_binary_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of human and machine predictions\n",
    "count = 0 \n",
    "for value in y_pred_binary_final:\n",
    "    if value == 0:\n",
    "        count +=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the final answer \n",
    "id = []\n",
    "for i in range(0,1000):\n",
    "    id.append(i)\n",
    "\n",
    "answer = []\n",
    "for i in range(0,1000):\n",
    "    answer.append((id[i],y_pred_binary_final[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to ../Predicted_answer/lgb_bow_balance_version21.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# write the answer into file \n",
    "file_name = '../Predicted_answer/lgb_bow_balance_version21.csv'\n",
    "column_name = ['id','class']\n",
    "with open(file_name, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(column_name)\n",
    "    for row in answer:\n",
    "        writer.writerow(row)\n",
    "print(f'Data has been written to {file_name}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
